import numpy as np
from replace_and_camelSplit import replace_a_to_b, split_c_and_s
# from get_bleu import return_bleu
from get_encoder import return_encoder
import bleu
import torch
import os
import logging
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm, trange
from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler, TensorDataset
from torch.utils.data.distributed import DistributedSampler
from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,
                          RobertaConfig, RobertaModel, RobertaTokenizer)

logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)
logger = logging.getLogger(__name__)
UNK = '<unk>'


def read_examples(idx, code, nl):
    """Read examples from filename."""
    examples = []
    examples.append(
        Example(
            idx=idx,
            source=code,
            target=nl,
        )
    )
    return examples


class InputFeatures(object):
    """A single training/test features for a example."""

    def __init__(self,
                 input_tokens,
                 input_ids,
                 attention_mask,
                 idx,
                 label,

                 ):
        self.input_tokens = input_tokens
        self.input_ids = input_ids
        self.attention_mask = attention_mask
        self.idx = str(idx)
        self.label = label


def convert_examples_to_features(code, idx,target, tokenizer, args):
    # source
    code = ' '.join(code.split())
    code_tokens = tokenizer.tokenize(code)[:args.block_size - 2]
    source_tokens = ["<|endoftext|>"] + code_tokens + ["<|endoftext|>"]
    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)
    padding_length = args.block_size - len(source_ids)
    source_ids += [50255] * padding_length
    attention_mask = (source_ids != 0)
    return InputFeatures(source_tokens, source_ids, attention_mask, idx, int(target))


class TextDataset(Dataset):
    def __init__(self,code, idx,target, tokenizer, args, file_path=None):
        self.examples = []

        self.examples.append(convert_examples_to_features(code, idx,target, tokenizer, args))

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return torch.tensor(self.examples[i].input_ids), torch.tensor(self.examples[i].label)


def computer_word_saliency_cos(model, code, summary, variable_list, vocab, embeddings, max_token,
                               vocab_src, vocab_trg, max_token_src, max_token_trg):
    word_saliency_list = []
    code_ = split_c_and_s(code.split(' '))
    # _,encoder=return_bleu(code_,summary,model)

    encoder = return_encoder(code_, summary, model, vocab_src, vocab_trg, max_token_src, max_token_trg)

    for var in variable_list:
        var_index = vocab[var] if var in vocab else max_token
        embedding_var = embeddings[var_index]

        cos = cosine_similarity(embedding_var.reshape(1, 64), encoder.reshape(1, 64))[0][0]
        cos = (1.0 + cos) * 0.5
        word_saliency_list.append((var, cos))
    return word_saliency_list


def return_bleu(args, model, eval_dataset):
    query_cnt = 0
    orig_prob = []
    orig_label = []
    for index, example in enumerate(eval_dataset):
        orig_prob, orig_label = model.get_results([example], args.eval_batch_size)
        query_cnt += 1

    return query_cnt, orig_prob, orig_label


def computer_best_substitution(model, code, target, variable_list, nearest_k_dict, args, tokenizer, device, epoch,
                               count):
    query_cnt = 0
    tmp_cnt = 0
    best_substitution_list = []
    code_=split_c_and_s(code.split(' '))
    tmp_code = code
    # print(code_)
    # print("old_bleu")
    # print("code", code_)
    # print("code2",code2)
    # print("target", target)
    print("count", count)
    target = target.replace("\n", "")
    # eval_examples = read_examples(count,code_,summary)
    eval_dataset = TextDataset(code_, count, target, tokenizer, args)
    tmp_cnt, orig_prob, orig_label = return_bleu(args, model, eval_dataset)
    query_cnt += tmp_cnt

    new_variable_list = []
    for var in variable_list:  # variable list  for one code
        new_variable_list.append(var)
        print("new_bleu")
        print("var", var)
        max_delta_bleu = 0
        nearest_k = nearest_k_dict[var]  # a list
        best_new_var = nearest_k[0]
        for new_var in nearest_k:
            print("new_var", new_var)
            new_code_list = replace_a_to_b(code, var, new_var)
            new_code = split_c_and_s(new_code_list)
            # print("new_code", new_code)
            eval_dataset = TextDataset(new_code, count, target, tokenizer, args)
            tmp_cnt, orig_prob_, orig_label_ = return_bleu(args, model, eval_dataset)
            query_cnt += tmp_cnt
            # new_bleu,_=return_bleu(new_code,summary,model)
            # print("orig_prob_: ", orig_prob_)
            # print("orig_label_: ", orig_label_)
            if orig_label != orig_label_:
                best_new_var = new_var
                max_delta_bleu = 1
                break
            else:
                delta_bleu = orig_prob[0][orig_label_[0]] - orig_prob_[0][orig_label_[0]]
                if max_delta_bleu < delta_bleu:
                    max_delta_bleu = delta_bleu
                    best_new_var = new_var
        best_substitution_list.append((var, best_new_var, max_delta_bleu))

        tmp_code_list = replace_a_to_b(tmp_code, var, best_new_var)
        tmp_code = split_c_and_s(tmp_code_list)

        eval_dataset = TextDataset(tmp_code, count, target, tokenizer, args)
        tmp_cnt, orig_prob_, orig_label_ = return_bleu(args, model, eval_dataset)
        query_cnt += tmp_cnt
        if orig_label_ != orig_label:
            break
    return best_substitution_list, query_cnt, new_variable_list