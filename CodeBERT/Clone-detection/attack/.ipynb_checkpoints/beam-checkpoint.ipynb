{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../../../')\n",
    "sys.path.append('../code')\n",
    "sys.path.append('../../../Char_data')\n",
    "sys.path.append('../../../python_parser')\n",
    "retval = os.getcwd()\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "import pickle\n",
    "import copy\n",
    "import torch\n",
    "import multiprocessing\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from beamAttack import Beam_Atack\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "from run_parser import get_identifiers, get_example\n",
    "from model import Model\n",
    "from utils import set_seed\n",
    "from utils import Recorder, is_valid_variable_java\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler, TensorDataset\n",
    "from attacker import get_code_pairs, convert_examples_to_features\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import (RobertaConfig, RobertaModel, RobertaTokenizer)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)  # Only report warning\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(code1_tokens, code2_tokens, label, url1, url2, tokenizer, cache):\n",
    "    # source\n",
    "    code1_tokens = code1_tokens[:block_size - 2]\n",
    "    code1_tokens = [tokenizer.cls_token] + code1_tokens + [tokenizer.sep_token]\n",
    "    code2_tokens = code2_tokens[:block_size - 2]\n",
    "    code2_tokens = [tokenizer.cls_token] + code2_tokens + [tokenizer.sep_token]\n",
    "\n",
    "    code1_ids = tokenizer.convert_tokens_to_ids(code1_tokens)\n",
    "    padding_length = block_size - len(code1_ids)\n",
    "    code1_ids += [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "    code2_ids = tokenizer.convert_tokens_to_ids(code2_tokens)\n",
    "    padding_length = block_size - len(code2_ids)\n",
    "    code2_ids += [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "    source_tokens = code1_tokens + code2_tokens\n",
    "    source_ids = code1_ids + code2_ids\n",
    "    return InputFeatures(source_tokens, source_ids, label, url1, url2)\n",
    "\n",
    "def get_example1(item):\n",
    "    url1, url2, label, tokenizer, cache, url_to_code = item\n",
    "    if url1 in cache:\n",
    "        code1 = cache[url1].copy()\n",
    "    else:\n",
    "        try:\n",
    "            code = ' '.join(url_to_code[url1].split())\n",
    "        except:\n",
    "            code = \"\"\n",
    "        code1 = tokenizer.tokenize(code)\n",
    "    if url2 in cache:\n",
    "        code2 = cache[url2].copy()\n",
    "    else:\n",
    "        try:\n",
    "            code = ' '.join(url_to_code[url2].split())\n",
    "        except:\n",
    "            code = \"\"\n",
    "        code2 = tokenizer.tokenize(code)\n",
    "\n",
    "    return convert_examples_to_features(code1, code2, label, url1, url2, tokenizer, cache)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path='train', block_size=512, pool=None):\n",
    "        postfix = file_path.split('/')[-1].split('.txt')[0]\n",
    "        self.examples = []\n",
    "        index_filename = file_path\n",
    "        logger.info(\"Creating features from index file at %s \", index_filename)\n",
    "        url_to_code = {}\n",
    "        with open('/'.join(index_filename.split('/')[:-1]) + '/data.jsonl') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                js = json.loads(line)\n",
    "                url_to_code[js['idx']] = js['func']\n",
    "\n",
    "        data = []\n",
    "        cache = {}\n",
    "        with open(index_filename) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                url1, url2, label = line.split('\\t')\n",
    "                if url1 not in url_to_code or url2 not in url_to_code:\n",
    "                    continue\n",
    "                if label == '0':\n",
    "                    label = 0\n",
    "                else:\n",
    "                    label = 1\n",
    "                data.append((url1, url2, label, tokenizer, args, cache, url_to_code))\n",
    "        # if 'test' not in postfix:\n",
    "        #     data = random.sample(data, int(len(data) * 0.1))\n",
    "\n",
    "        self.examples = pool.map(get_example1, tqdm(data, total=len(data)))\n",
    "        if 'train' in postfix:\n",
    "            for idx, example in enumerate(self.examples[:3]):\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"idx: {}\".format(idx))\n",
    "                logger.info(\"label: {}\".format(example.label))\n",
    "                logger.info(\"input_tokens: {}\".format([x.replace('\\u0120', '_') for x in example.input_tokens]))\n",
    "                logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        return torch.tensor(self.examples[item].input_ids), torch.tensor(self.examples[item].label)\n",
    "\n",
    "def tokenize_with_camel_case(token):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', token)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "\n",
    "def tokenize_with_snake_case(token):\n",
    "    return token.split('_')\n",
    "\n",
    "def is_camel_case(s):\n",
    "    transfer = tokenize_with_camel_case(s)[0]\n",
    "    return False if transfer == s else True\n",
    "\n",
    "def is_snake_case(s):\n",
    "    return True if '_' in s else False\n",
    "\n",
    "def get_advs(split_identifiers, sub_tokens, nodigit=True):\n",
    "    adv_vars = []\n",
    "    replace_tokens = []\n",
    "    for token in sub_tokens:\n",
    "        similars = []\n",
    "        num = 0\n",
    "        distence = 1\n",
    "        for x in split_identifiers:\n",
    "            if Levenshtein.distance(token, x) <= distence and num < 30:\n",
    "                num += 1\n",
    "                similars.append(x)\n",
    "        while num < 30:\n",
    "            distence += 1\n",
    "            for x in split_identifiers:\n",
    "                if Levenshtein.distance(token, x) <= distence and num < 30:\n",
    "                    num += 1\n",
    "                    similars.append(x)\n",
    "        similars = [token] if len(similars) == 0 else similars\n",
    "        if token.isdigit():\n",
    "            similars = [token]\n",
    "        replace_tokens.append(similars)\n",
    "    result = [[]]\n",
    "    for list_pool in replace_tokens:\n",
    "        lis = []\n",
    "        for i in result:\n",
    "            for j in list_pool:\n",
    "                lis.append(i + [j])\n",
    "        result = lis\n",
    "    for adv_var in result:\n",
    "        if nodigit:\n",
    "            adv_var_snake = [i.lower() for i in adv_var]\n",
    "            adv_vars.append('_'.join(adv_var_snake))\n",
    "        adv_var_camel = [i.title() for i in adv_var]\n",
    "        adv_vars.append(''.join(adv_var_camel))\n",
    "        adv_var_pascal = [i.title() for i in adv_var]\n",
    "        adv_var_pascal[0] = adv_var_pascal[0].lower()\n",
    "        adv_vars.append(''.join(adv_var_pascal))\n",
    "    return adv_vars\n",
    "\n",
    "def get_new_substituions(split_identifiers, identifiers):\n",
    "    new_substituions = {}\n",
    "    for var in identifiers:\n",
    "        adv_vars = []\n",
    "        if is_camel_case(var):\n",
    "            sub_tokens = tokenize_with_camel_case(var)\n",
    "            adv_vars = get_advs(split_identifiers, sub_tokens)\n",
    "\n",
    "        elif is_snake_case(var):\n",
    "            sub_tokens = tokenize_with_snake_case(var)\n",
    "            adv_vars = get_advs(split_identifiers, sub_tokens)\n",
    "        elif bool(re.search(r'\\d', var)):\n",
    "            sub_tokens = re.findall(r'\\d+|(?:[^\\w\\s]|_)+|[^\\W\\d_]+', var)\n",
    "            adv_vars = get_advs(split_identifiers, sub_tokens, nodigit=False)\n",
    "\n",
    "        org_similars = []\n",
    "        num = 0\n",
    "        distence = 1\n",
    "        for x in split_identifiers:\n",
    "            if Levenshtein.distance(var, x) <= distence and num < 30:\n",
    "                num += 1\n",
    "                org_similars.append(x)\n",
    "        while num < 30:\n",
    "            distence += 1\n",
    "            for x in split_identifiers:\n",
    "                if Levenshtein.distance(var, x) <= distence and num < 30:\n",
    "                    num += 1\n",
    "                    org_similars.append(x)\n",
    "        adv_vars1 = [var] if len(org_similars) == 0 else org_similars\n",
    "        adv_vars = adv_vars + adv_vars1\n",
    "\n",
    "        adv_vars = [var_tmp for var_tmp in adv_vars if var_tmp != var]\n",
    "        adv_vars = list(set(adv_vars[:30]))\n",
    "        if len(adv_vars) > 0:\n",
    "            new_substituions[var] = adv_vars\n",
    "    return new_substituions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[block_size] = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-55684da135c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                             \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                             cache_dir=None)\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'checkpoint-best-f1/model.bin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'args'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "config = RobertaConfig.from_pretrained(\"microsoft/codebert-base\")\n",
    "config.num_labels = 2\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\", do_lower_case=False, cache_dir=None)\n",
    "block_size = min(512, tokenizer.max_len_single_sentence)\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\",\n",
    "                                            from_tf=bool('.ckpt' in \"microsoft/codebert-base\"),\n",
    "                                            config=config,\n",
    "                                            cache_dir=None)\n",
    "model = Model(model, config, tokenizer)\n",
    "checkpoint_prefix = 'checkpoint-best-f1/model.bin'\n",
    "output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
    "model.load_state_dict(torch.load(output_dir))\n",
    "model.to(device)\n",
    "logger.info(\"reload model from {}\".format(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
